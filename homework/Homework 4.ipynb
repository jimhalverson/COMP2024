{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: ML for Physics 1\n",
    "\n",
    "Due Dec 3, enjoy.\n",
    "\n",
    "You might find problem 2 challenging, start soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Simple neural network for parameter inference\n",
    "\n",
    "An underdamped harmonic oscillator has solution\n",
    "\n",
    "$x(t)=A\\exp(-\\beta t) \\cos(\\omega t - \\delta)$.\n",
    "\n",
    "Sometimes in physics we have a good theory -- such as in this case -- that we trust. The problem, then, is an experimental one: given some experimental data, what are the values of the parameters in the theory that account for it? In this case, we will want to take some data and infer the values of $A$, $\\beta$, $\\omega$, and $\\delta$.\n",
    "\n",
    "Write a code using a NN library, e.g. torch or jax, that:\n",
    "1. uses the equation above as a simple neural network, meaning the parameters are *learnable parameters*.\n",
    "2. trains the network with MSE loss on the given data.\n",
    "3. infer (print) the parameters $A$, $\\beta$, $\\omega$, and $\\delta$ from the trained network and compare them to the ground-truth values.\n",
    "4. plot the data and the inferred solution imposed on it. Comment on the goodness of fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "num_samples = 100\n",
    "time_end = 5\n",
    "times = np.linspace(0, time_end, num_samples)\n",
    "x_data = np.array([ 1.42136407e+00,  1.32538052e+00,  1.08997205e+00,  4.01375320e-01,\n",
    "       -1.78495795e-01, -8.50696683e-01, -1.18615104e+00, -1.42918430e+00,\n",
    "       -1.08653056e+00, -5.51030603e-01, -7.50736710e-02,  6.47062767e-01,\n",
    "        1.01484947e+00,  1.23909135e+00,  1.16973363e+00,  7.14521669e-01,\n",
    "        3.55287429e-01, -2.95169206e-01, -9.74425168e-01, -1.16492314e+00,\n",
    "       -9.52631079e-01, -7.77656938e-01, -3.28057458e-01,  2.92744065e-01,\n",
    "        6.01953380e-01,  8.65371671e-01,  9.54240091e-01,  7.08483475e-01,\n",
    "        4.68455172e-01, -1.56245101e-01, -3.70602686e-01, -8.30733462e-01,\n",
    "       -8.95167164e-01, -8.91137955e-01, -4.18014142e-01, -2.22376053e-01,\n",
    "        4.10047775e-01,  6.88406843e-01,  1.04061990e+00,  7.30025281e-01,\n",
    "        3.90618018e-01,  1.43181366e-01, -2.20778546e-01, -4.53298514e-01,\n",
    "       -7.87562022e-01, -7.21285197e-01, -6.27664217e-01, -4.65873073e-01,\n",
    "        4.76786963e-02,  3.89328316e-01,  5.92877625e-01,  6.37519856e-01,\n",
    "        4.83785342e-01,  1.71821980e-01,  5.43082539e-02, -3.29509591e-01,\n",
    "       -4.12974420e-01, -5.53475010e-01, -5.83206553e-01, -4.58252882e-01,\n",
    "       -5.48980467e-02,  2.26195369e-01,  4.52451610e-01,  5.38430311e-01,\n",
    "        3.67845822e-01,  3.97176254e-01,  1.84432283e-01, -1.49708735e-01,\n",
    "       -5.37683940e-01, -3.88877197e-01, -6.79843289e-01, -4.05592818e-01,\n",
    "       -2.06048677e-01, -3.04612159e-02,  2.73534969e-01,  2.33194682e-01,\n",
    "        4.87619039e-01,  3.34767833e-01,  3.00019677e-01,  9.12268252e-03,\n",
    "       -1.21252257e-01, -2.57336778e-01, -4.68953016e-01, -5.12673830e-01,\n",
    "       -2.89839003e-02, -3.22855227e-01,  5.86021068e-02,  8.32512072e-02,\n",
    "        2.60554538e-01,  3.23171785e-01,  4.29277758e-01,  8.75180664e-02,\n",
    "       -1.17562189e-03, -5.57391100e-02, -2.43060644e-01, -3.07899430e-01,\n",
    "       -1.78936431e-01, -2.53676100e-01, -9.54204646e-02,  2.55114941e-01])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Phase Classification\n",
    "\n",
    "Can neural networks detect phase transitions? We'll test this idea in this problem, using a well-studied case where we understand everything.\n",
    "\n",
    "In class we solved the 2d Ising model and saw a phase transitions at $T_c\\sim 2.6K$. Below this critical temperature we were in the ordered phase, and above it we were in the disordered phase.\n",
    "\n",
    "In this problem we would like to learn to distinguish Ising samples from well below or above the critical temperature. To do so:\n",
    "1. Use the Monte Carlo algorithm *from class* to generate $10,000$ samples at both $T=0.5K$ and $T=5.5K$. Label these samples $0$ for ordered and $1$ for disordered.\n",
    "2. With $90/10$ train-test split, use a NN (probably CNN is best, but feel free to experiment) to classify the samples as ordered or disordered, plotting both train and validation (test) accuracy as a function of training epoch.\n",
    "3. We want to test whether the **already trained** NN can detect the critical temperature. To do so, generate $10,000$ samples for all $T$ in $[1.0,1.5,2.0,2.5,3.0,3.5.,4.0,4.5,5.0]$ and plot the average network prediction and error bars on these samples as a function of $T$. Does it see the phase transition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: **Extra Credit / Level Up**, Galaxy Classification\n",
    "\n",
    "In class, we classified galaxies as spiral, elliptical, or unknown using a convolutional neural network.\n",
    "\n",
    "In this problem, we would like to get a sense of how architecture differences affect performance. In class, we did the principled architecture of image data, a CNN, but we want to see if performance goes down by using another architecture instead. Specifically, we would like to compare performance of a CNN vs. depth $1$ feedforward neural network with ReLU non-linearity. I recommend using PyTorch for this.\n",
    "\n",
    "1. Modify the code from class to utilize a feedforward neural network with ReLU non-linearity. Note that you will have to *squish* the image output to instead be one vector, therefore removing all the local information from the image that CNNs learn from. At the end, run `print(model)`, where `model = YourModelClass()`.\n",
    "2. Change the width so that the number of parameters of this FFNN is *roughly* equal to that of the CNN from class. Alternatively, scale up the size of the CNN so that it has roughly the same number of parameters as the FFNN. Trainable parameters in PyTorch can be computed using `sum(p.numel() for p in model.parameters() if p.requires_grad)`. Print the number of trainable parameters for both models.\n",
    "3. Write a training loop for training the NN on the same training data as the CNN, with a variety of train-test splits for both [(90%,10%), (75%,25%), (50%,50%), (25%, 75%), (10%, 90%)]. You should keep track of / plot your train loss as a function of epochs so you know when to appropriately stop training. IMPORTANT: if you run into problems during training, such as `nan` or outrageous / boring predictions, it could be that your network is suffering from *vanishing or exploding gradients*. This can be mitigated by tuning your learning rate if it's too small or too large, adding `weight_decay` in Adam, and / or adding batch normalization (see PyTorch docs).\n",
    "4. Make one plot for each **label**. On each plot, show the test-set accuracy of the CNN and the FFNN as a function of the train-test split, with different colors for the different architectures. Display these plots nicely in a grid. Comment on if you see any trends.\n",
    "\n",
    "Note: PyTorch requires you setting a `device` to perform computation on. If running on an Apple ARM machine, use: `device = torch.device(\"mps\" if torch.backends.mps.is_available else \"cpu\")`. If running on a local GPU or on discovery, use: `device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
