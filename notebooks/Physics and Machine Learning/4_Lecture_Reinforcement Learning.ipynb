{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "37244cd7",
            "metadata": {},
            "source": [
                "# Reinforcement Learning\n",
                "\n",
                "Today's topic, Reinforcement Learning, is closed to what you might have traditionally meant by \"artificial intelligence,\" which maybe isn't a word that you would necessarily apply to a CNN that labels pictures of galaxies (though maybe you would.) When we thing of artificial intelligence, we think of a computer that can learn to play chess, or a robot that can learn to walk. Reinforcement learning is the field of machine learning that deals with these kinds of problems.\n",
                "\n",
                "The essential ideas of RL are:\n",
                "- an *agent* is exploring an *environment.*\n",
                "- at any given time, the agent perceives a *state* of the environment, $s\\in S$.\n",
                "- the agent can take *actions* that change the state of the environment, $a\\in A$.\n",
                "- but how does the agent choose? via a *policy function* $\\pi:S\\rightarrow A$. The policy might be deterministic, or it might be stochastic, an $s$-dependent probability density on $A$.\n",
                "- by following the policy, the agent generates a trajectory through state space, that terminates either at some fixed timeout or when the agent reaches a *terminal state,* characterized by an end goal like checkmate.\n",
                "- at each time step, the agent receives a *reward* $r_t$ that depends on the state, which accumulate into a *return*\n",
                "    $$G_t = \\sum_{k=0}^\\infty \\gamma^k r_{t+k+1}$$\n",
                "    where $\\gamma\\in[0,1]$ is a *discount factor* that determines how much the agent cares about the future.\n",
                "- the expected return given a state is the *value function* a.k.a. *state value function* of a state, \n",
                "        $$v_\\pi(s) = \\mathbb{E}_\\pi[G_t|S_t=s],$$\n",
                "  and the expected return given a state-action pair is the *action value function*,\n",
                "        $$q_\\pi(s,a) = \\mathbb{E}_\\pi[G_t|S_t=s,A_t=a].$$\n",
                "\n",
                "The goal of RL is to find a policy that maximizes the expected return. Some algorithms do this by estimating the value function or action-value function, and then using it to find the optimal policy. These are called *value-based* algorithms. Others directly estimate the optimal policy. These are called *policy-based* methods, or policy gradients.\n",
                "\n",
                "As a litmus test for whether RL is right for your problem, you should ask whether it naturally admits a description in terms of an agent exploring an environment, and taking particular actions to do so. Also necessary is a notion of what is \"good\" or \"bad\" in the environment, to set up your reward. If the state space is small enough, the problem can be solved by going through all the states brute-force. However, in cases with exponentially large state spaces, such as Chess or Go or String Theory, we can't go through all the states and instead use techniques that approximate the value function or policy. In practice, those techniques are neural networks, and the associated RL techniques are called *deep reinforcement learning.*\n",
                "\n",
                "For more on RL, see:\n",
                "- [Sutton and Barto](http://incompleteideas.net/book/the-book-2nd.html), a famous early textbook.\n",
                "- [David Silver's course](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html), a more modern take. The videos were my entry point into RL.\n",
                "- [AlphaZero](https://www.nature.com/articles/nature24270), a 2017 breakthrough in which RL achieves superhuman gameplay in Go *without human knowledge*, i.e. only via knowledge of the game and self-play. It was extended to Chess and Shogi in 2018. See the arXiv article here: [Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/abs/1712.01815).\n",
                "- My friends and I introduced RL into string theory in [Branes with Brains](https://arxiv.org/abs/1903.11616). We have also used it to [find unknots](https://arxiv.org/abs/2010.16263) and [ribbons](https://arxiv.org/abs/2304.09304), the latter in connection with the smooth 4d Poincar\u00e9 conjecture.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Gridworld\n",
                "\n",
                "In this class we'll study a famous game in RL called *Gridworld*, see. e.g. Sutton and Barto for more. It's a simple game that is easy to understand, but still has some interesting features. The game is played on a grid, and the agent can move up, down, left, or right. The agent starts in a random position, and the goal is to reach the goal state, which is chosen via the policy.\n",
                "The game ends when the agent reaches the goal."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a3c19aaa",
            "metadata": {},
            "source": [
                "First we'll set up some helper functions that will be used below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2c25e224",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "ec36fc3c",
            "metadata": {},
            "source": [
                "### Defining Gridworld\n",
                "This module defines the Gridworld game environment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "69aabb44",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "1ea1e76f",
            "metadata": {},
            "source": [
                "Now we have the agent explore Gridworld. Since the state space is small, we can explore the entire environment and find optimal policies. We will do so using an algorithm called *SARSA*. The basic idea is to estimate the action-value function $q_\\pi(s,a)$, and then use it to find the optimal policy. The algorithm is as follows:\n",
                "- Initialize $Q(s,a)$ arbitrarily.\n",
                "- "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "efadae12",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "a5456ee4",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "This notebook demonstrates the integration and functionality of a Gridworld game environment using Python."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "comp2024",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}